# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
lb
glusterfs

# Set variables common for all OSEv3 hosts
[OSEv3:vars]

# If ansible_ssh_user is not root, ansible_sudo must be set to true
ansible_ssh_user=root
#ansible_sudo=true
#ansible_become=yes

# Install Enterprise or Origin; set up ntp
openshift_deployment_type=openshift-enterprise
openshift_clock_enabled=true

# Network/DNS Related
openshift_master_default_subdomain=apps.cloud.chx
osm_cluster_network_cidr=10.1.0.0/16
osm_host_subnet_length=8
openshift_portal_net=172.30.0.0/16
openshift_docker_insecure_registries=172.30.0.0/16
#container_runtime_docker_storage_setup_device=/dev/nvme1n1

# CNS Storage
openshift_storage_glusterfs_namespace=glusterfs
openshift_storage_glusterfs_name=storage
openshift_storage_glusterfs_heketi_wipe=true
openshift_storage_glusterfs_wipe=true
openshift_storage_glusterfs_storageclass_default=true
openshift_storage_glusterfs_block_storageclass=true
openshift_storage_glusterfs_block_host_vol_size=50

# Automatically Deploy the router
openshift_hosted_manage_router=true
#openshift_router_selector={'node-role.kubernetes.io/infra':'true'}

# Automatically deploy the registry using glusterfs
openshift_hosted_manage_registry=true
openshift_hosted_registry_storage_kind=glusterfs
openshift_hosted_registry_storage_volume_size=25Gi
#openshift_registry_selector={'node-role.kubernetes.io/infra':'true'}
#openshift_hosted_registry_replicas=2

# Disble Checks
openshift_disable_check=disk_availability,docker_storage,memory_availability

# 
# Network Policies that are available:
# redhat/openshift-ovs-networkpolicy # fine grained control
# redhat/openshift-ovs-subnet # "flat" network
#
# Mulititenant functionality (i.e. each project gets it's own "private" network)
os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'

# Uncomment when setting up logging/metrics/prometheus
openshift_master_dynamic_provisioning_enabled=true
dynamic_volumes_check=False

# Logging
openshift_logging_install_logging=true
openshift_logging_es_pvc_dynamic=true
openshift_logging_es_pvc_size=20Gi
openshift_logging_es_pvc_storage_class_name=glusterfs-storage-block
openshift_logging_curator_nodeselector={'node-role.kubernetes.io/infra':'true'}
openshift_logging_es_nodeselector={'node-role.kubernetes.io/infra':'true'}
openshift_logging_kibana_nodeselector={'node-role.kubernetes.io/infra':'true'}
openshift_logging_es_memory_limit=4G

# Metrics
openshift_metrics_install_metrics=true
openshift_metrics_cassandra_storage_type=dynamic
openshift_metrics_cassandra_pvc_size=20i
openshift_metrics_cassanda_pvc_storage_class_name=glusterfs-storage-block
openshift_metrics_hawkular_nodeselector={'node-role.kubernetes.io/infra':'true'}
openshift_metrics_heapster_nodeselector={'node-role.kubernetes.io/infra':'true'}
openshift_metrics_cassandra_nodeselector={'node-role.kubernetes.io/infra':'true'}

# Ansible Service Broker
openshift_hosted_etcd_storage_kind=dynamic
openshift_hosted_etcd_storage_volume_name=etcd-vol2
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=10Gi
ansible_service_broker_local_registry_whitelist=['.*-apb$']

# If using Route53 or you're pointed to the master with a "vanity" name
openshift_master_public_api_url=https://ocp.cloud.chx:8443
openshift_master_public_console_url=https://ocp.cloud.chx:8443/console
openshift_master_api_port=8443
openshift_master_console_port=8443

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has 
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=ocp.cloud.chx
openshift_master_cluster_public_hostname=ocp.cloud.chx

# The following enabled htpasswd authentication
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]
openshift_master_htpasswd_users={'developer': '$apr1$q2fVVf46$85HP/4JHGYeFBKAKPBblo0'}

## OpenShift host groups

# host group for etcd
[etcd]
master[1:3].cloud.chx

# host group for loadbalancers
[lb]
ocp.cloud.chx

# host group for masters - set scedulable to "true" for the web-console pod
[masters]
master[1:3].cloud.chx openshift_schedulable=true

# host group for nodes, includes region info
[nodes]
master1.cloud.chx openshift_node_group_name='node-config-master'
master2.cloud.chx openshift_node_group_name='node-config-master'
master3.cloud.chx openshift_node_group_name='node-config-master'
infra1.cloud.chx openshift_node_group_name='node-config-infra'
infra2.cloud.chx openshift_node_group_name='node-config-infra'
infra3.cloud.chx openshift_node_group_name='node-config-infra'
app1.cloud.chx openshift_node_group_name='node-config-compute'
app2.cloud.chx openshift_node_group_name='node-config-compute'
app3.cloud.chx openshift_node_group_name='node-config-compute'

[glusterfs]
# "standalone" glusterfs nodes STILL need to be in the "[nodes]" section
app1.cloud.chx glusterfs_ip=192.168.1.32 glusterfs_zone=1 glusterfs_devices='[ "/dev/vdc" ]'
app2.cloud.chx glusterfs_ip=192.168.1.42 glusterfs_zone=2 glusterfs_devices='[ "/dev/vdc" ]'
app3.cloud.chx glusterfs_ip=192.168.1.52 glusterfs_zone=3 glusterfs_devices='[ "/dev/vdc" ]'
##
##


